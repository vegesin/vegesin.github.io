<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>量化感知训练</title>
    <url>/2025/12/09/%E9%87%8F%E5%8C%96%E6%84%9F%E7%9F%A5%E8%AE%AD%E7%BB%83/</url>
    <content><![CDATA[<h1 id="量化感知训练"><a href="#量化感知训练" class="headerlink" title="量化感知训练"></a>量化感知训练</h1><h2 id="1-摘要"><a href="#1-摘要" class="headerlink" title="1.摘要"></a>1.摘要</h2><p>模型量化通过将模型权重或激活值从高精度的浮点数表示转换为低精度的整数表示，来减小模型大小、降低内存占用，同时在推理时将浮点的矩阵运算转换为整型运算，以此进行加速。</p>
<p>本文重点介绍使用torch框架进行量化感知训练，同时提取量化参数进行量化推理。</p>
<h2 id="2-量化基础原理"><a href="#2-量化基础原理" class="headerlink" title="2.量化基础原理"></a>2.量化基础原理</h2><p>模型量化的核心思想是将连续的浮点数值映射到有限的、离散的整数值集合。量化的核心是建立 FP32 与 INT8 数值之间的一一对应关系。这种映射关系通常采用仿射变换实现，其中$q$表示整形，$r$表示浮点，$S$为缩放因子(Scale)，$Z$表示零点(Zero-Point):</p>
<p>$$<br>\begin{align*}<br>q &amp;&#x3D; \textbf{round}(S \times r) + Z \\<br>r &amp;&#x3D; S \times (q - Z)<br>\end{align*}<br>$$</p>
<p>以卷积运算为例子，输入$X$，卷积权重$W$，输出$Y$：</p>
<p>$$<br>\begin{align*}<br>Y_r &amp;&#x3D; X_r * W_r \<br>Y_S \times (Y_q - Y_z) &amp;&#x3D; [X_S \times (X_q - X_z)] * [W_S \times (W_q - W_z)] \\<br>Y_q &amp;&#x3D; \frac{X_S W_S}{Y_S}[(X_q - X_z) * (W_q - W_z)] + Y_Z<br>\end{align*}<br>$$</p>
<p>根据上式，使用量化之后的整形表示浮点运算的关键是得到缩放因子$S$和零点$Z$这两个量化参数。</p>
<p>在实际的模型量化过程中，主流的量化方案包括训练后量化和量化感知训练。</p>
<h3 id="训练后量化"><a href="#训练后量化" class="headerlink" title="训练后量化"></a>训练后量化</h3><p>训练后量化(Post-Training Quantization，PTQ）是一种简单快捷的量化方法。它直接对一个已经训练好的浮点模型进行转换，无需重新训练。PTQ通常只需要少量校准数据集（calibration data）来确定权重和激活值的量化参数$S$和$Z$。</p>
<h3 id="量化感知训练-1"><a href="#量化感知训练-1" class="headerlink" title="量化感知训练"></a>量化感知训练</h3><p>量化感知训练 (Quantization-Aware Training, QAT) 通过在模型中插入伪量化节点，在训练过程中模拟量化操作带来的误差。在反向传播更新权重时，模型会将量化误差最为损失主动优化。</p>
<p>对一个模型进行QAT时，可以从零开始重新训练，也可以使用一个预训练的模型，通过在校准集上微调，学习出量化参数。</p>
<p>下面本文将重点介绍使用Torch进行QAT。</p>
<h2 id="3-QAT-Torch实现"><a href="#3-QAT-Torch实现" class="headerlink" title="3.QAT Torch实现"></a>3.QAT Torch实现</h2><h3 id="量化模型定义"><a href="#量化模型定义" class="headerlink" title="量化模型定义"></a>量化模型定义</h3><p>将一个浮点模型转换为一个量化模型，主要包含以下步骤：</p>
<ul>
<li>输入输出量化节点插入 ：在model_fp32的forward中输入添加 QuantStub() 输出添加DeQuantStub()</li>
<li>模型融合 ：需要将模型中的Conv BN等进行算子融合，无法BN进行单独量化 ，使用quantization.fuse_modules 进行算子融合，也可以手动实现。需要注意的是进行融合，模型要切换到eval()</li>
<li>插入量化节点 ：在原本的model_fp32中插入量化节点，quantization.prepare_qat(model_fused)</li>
<li>配置量化参数：qnnpack量化方案，qconfig &#x3D; quantization.get_default_qat_qconfig(‘qnnpack’)</li>
<li>模型训练：使用训练集测试集对量化模型进行正常训练</li>
<li>模型转换：使用quantization.convert(model_qat)将模型中的伪量化节点舍弃，模型权重保存为torch.qint8类型，该类型中包含权重的量化参数w_fp、w_int、w_scale、w_zero_point、scale、zero_point，这里模型也需要切换到eval()</li>
</ul>
<p>注意当网络存在残差连接的时候，使用FloatFunctional.add 替代 +</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch.ao.quantization <span class="keyword">import</span> QuantStub, DeQuantStub</span><br><span class="line"><span class="keyword">from</span> torch.ao.nn.quantized <span class="keyword">import</span> FloatFunctional</span><br><span class="line"></span><br><span class="line"><span class="comment"># 一个量化残差网络示例</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ResNet_resblock1_Quantized</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    一个集成了量化支持和融合逻辑的 ResNet 模型</span></span><br><span class="line"><span class="string">    直接修修改定义一个量化模型,不再使用model_fp32封装传入参数,这样不好进行模型的融合.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes=<span class="number">1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(ResNet_resblock1_Quantized, <span class="variable language_">self</span>).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 量化/反量化存根</span></span><br><span class="line">        <span class="variable language_">self</span>.quant = QuantStub()</span><br><span class="line">        <span class="variable language_">self</span>.dequant = DeQuantStub()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 1. 初始大幅度降采样层</span></span><br><span class="line">        <span class="variable language_">self</span>.initial_pool = nn.MaxPool2d(kernel_size=<span class="number">8</span>, stride=<span class="number">8</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2. 初始卷积层</span></span><br><span class="line">        <span class="variable language_">self</span>.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">8</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="variable language_">self</span>.bn1 = nn.BatchNorm2d(<span class="number">8</span>)</span><br><span class="line">        <span class="variable language_">self</span>.relu1 = nn.ReLU()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 3.残差连接层</span></span><br><span class="line">        <span class="variable language_">self</span>.layer1 = ResidualBlock(<span class="number">8</span>, <span class="number">16</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 4. 分类头</span></span><br><span class="line">        <span class="variable language_">self</span>.avg_pool = nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        <span class="variable language_">self</span>.flatten = nn.Flatten()</span><br><span class="line">        <span class="variable language_">self</span>.fc = nn.Linear(<span class="number">16</span>, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.quant(x) <span class="comment"># 输入量化</span></span><br><span class="line"></span><br><span class="line">        out = <span class="variable language_">self</span>.initial_pool(x)</span><br><span class="line"></span><br><span class="line">        out = <span class="variable language_">self</span>.relu1(<span class="variable language_">self</span>.bn1(<span class="variable language_">self</span>.conv1(out)))</span><br><span class="line"></span><br><span class="line">        out = <span class="variable language_">self</span>.layer1(out)</span><br><span class="line"></span><br><span class="line">        out = <span class="variable language_">self</span>.avg_pool(out)</span><br><span class="line">        out = <span class="variable language_">self</span>.flatten(out)</span><br><span class="line">        out = <span class="variable language_">self</span>.fc(out)</span><br><span class="line"></span><br><span class="line">        x = <span class="variable language_">self</span>.dequant(out) <span class="comment"># 输出反量化</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fuse_model</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        实现模型融合的核心方法。</span></span><br><span class="line"><span class="string">        这个方法直接在模型自身内部进行操作。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 1. 融合初始卷积块 (Conv + BN + ReLU)</span></span><br><span class="line">        <span class="comment"># 因为我们将 conv1, bn1, relu1 分开定义了，所以可以直接按名字融合</span></span><br><span class="line">        torch.quantization.fuse_modules(<span class="variable language_">self</span>, [<span class="string">&#x27;conv1&#x27;</span>, <span class="string">&#x27;bn1&#x27;</span>, <span class="string">&#x27;relu1&#x27;</span>], inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2. 递归地融合所有 ResidualBlock 子模块</span></span><br><span class="line">        <span class="keyword">for</span> module <span class="keyword">in</span> <span class="variable language_">self</span>.modules():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(module, ResidualBlock):</span><br><span class="line">                <span class="comment"># 融合主路径</span></span><br><span class="line">                torch.quantization.fuse_modules(module, [[<span class="string">&#x27;conv1&#x27;</span>, <span class="string">&#x27;bn1&#x27;</span>, <span class="string">&#x27;relu1&#x27;</span>], [<span class="string">&#x27;conv2&#x27;</span>, <span class="string">&#x27;bn2&#x27;</span>]], inplace=<span class="literal">True</span>)</span><br><span class="line">                <span class="comment"># 融合 shortcut 路径</span></span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">len</span>(module.shortcut) &gt; <span class="number">0</span>:</span><br><span class="line">                    torch.quantization.fuse_modules(module.shortcut, [<span class="string">&#x27;0&#x27;</span>, <span class="string">&#x27;1&#x27;</span>], inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span></span><br><span class="line">    </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ResidualBlock</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    一个简单的残差块实现。</span></span><br><span class="line"><span class="string">    如果输入和输出的维度不同（例如，通道数或尺寸），</span></span><br><span class="line"><span class="string">    则使用1x1卷积的下采样层来匹配维度。</span></span><br><span class="line"><span class="string">    将残差连接(&quot;+&quot;)替换为 torch.ao.nn.quantized.FloatFunctional.add(),进行残差连接的量化</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, out_channels, stride=<span class="number">1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(ResidualBlock, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment"># 主路径</span></span><br><span class="line">        <span class="variable language_">self</span>.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">3</span>, stride=stride, padding=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="variable language_">self</span>.bn1 = nn.BatchNorm2d(out_channels)</span><br><span class="line">        <span class="variable language_">self</span>.relu1 = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        <span class="variable language_">self</span>.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="variable language_">self</span>.bn2 = nn.BatchNorm2d(out_channels)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 快捷连接（Skip Connection）</span></span><br><span class="line">        <span class="variable language_">self</span>.shortcut = nn.Sequential()</span><br><span class="line">        <span class="keyword">if</span> stride != <span class="number">1</span> <span class="keyword">or</span> in_channels != out_channels:</span><br><span class="line">            <span class="comment"># 使用1x1卷积进行下采样和维度匹配</span></span><br><span class="line">            <span class="variable language_">self</span>.shortcut = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">1</span>, stride=stride, bias=<span class="literal">False</span>), nn.BatchNorm2d(out_channels))</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.add = FloatFunctional()</span><br><span class="line">        <span class="variable language_">self</span>.relu2 = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        out = <span class="variable language_">self</span>.relu1(<span class="variable language_">self</span>.bn1(<span class="variable language_">self</span>.conv1(x)))</span><br><span class="line">        out = <span class="variable language_">self</span>.bn2(<span class="variable language_">self</span>.conv2(out))</span><br><span class="line">        <span class="comment"># out += self.shortcut(x) # 加上快捷连接</span></span><br><span class="line">        out = <span class="variable language_">self</span>.add.add(out, <span class="variable language_">self</span>.shortcut(x)) <span class="comment"># 量化： 使用FloatFunctional.add 替代 +</span></span><br><span class="line">        out = <span class="variable language_">self</span>.relu2(out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>

<p>下面是Torch官方文档参考，初始化一个量化模型。在模型的前向传播的输入输出中插入量化节点，将模型进行算子融合，配置量化方案，调用torch.ao.quantization.prepare_qat() 插入量化节点，之后进行和浮点模型一样的训练，训练完成之后调用 torch.ao.quantization.convert()再将模型转换为一个量化模型进行权重保存。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># define a floating point model where some layers could benefit from QAT</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">M</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># QuantStub converts tensors from floating point to quantized</span></span><br><span class="line">        <span class="variable language_">self</span>.quant = torch.ao.quantization.QuantStub()</span><br><span class="line">        <span class="variable language_">self</span>.conv = torch.nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="variable language_">self</span>.bn = torch.nn.BatchNorm2d(<span class="number">1</span>)</span><br><span class="line">        <span class="variable language_">self</span>.relu = torch.nn.ReLU()</span><br><span class="line">        <span class="comment"># DeQuantStub converts tensors from quantized to floating point</span></span><br><span class="line">        <span class="variable language_">self</span>.dequant = torch.ao.quantization.DeQuantStub()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.quant(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.conv(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.bn(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.relu(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.dequant(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># create a model instance</span></span><br><span class="line">model_fp32 = M()</span><br><span class="line"></span><br><span class="line"><span class="comment"># model must be set to eval for fusion to work</span></span><br><span class="line">model_fp32.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># attach a global qconfig, which contains information about what kind</span></span><br><span class="line"><span class="comment"># of observers to attach. Use &#x27;x86&#x27; for server inference and &#x27;qnnpack&#x27;</span></span><br><span class="line"><span class="comment"># for mobile inference. Other quantization configurations such as selecting</span></span><br><span class="line"><span class="comment"># symmetric or asymmetric quantization and MinMax or L2Norm calibration techniques</span></span><br><span class="line"><span class="comment"># can be specified here.</span></span><br><span class="line"><span class="comment"># Note: the old &#x27;fbgemm&#x27; is still available but &#x27;x86&#x27; is the recommended default</span></span><br><span class="line"><span class="comment"># for server inference.</span></span><br><span class="line"><span class="comment"># model_fp32.qconfig = torch.ao.quantization.get_default_qconfig(&#x27;fbgemm&#x27;)</span></span><br><span class="line">model_fp32.qconfig = torch.ao.quantization.get_default_qat_qconfig(<span class="string">&#x27;x86&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># fuse the activations to preceding layers, where applicable</span></span><br><span class="line"><span class="comment"># this needs to be done manually depending on the model architecture</span></span><br><span class="line">model_fp32_fused = torch.ao.quantization.fuse_modules(model_fp32,</span><br><span class="line">    [[<span class="string">&#x27;conv&#x27;</span>, <span class="string">&#x27;bn&#x27;</span>, <span class="string">&#x27;relu&#x27;</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Prepare the model for QAT. This inserts observers and fake_quants in</span></span><br><span class="line"><span class="comment"># the model needs to be set to train for QAT logic to work</span></span><br><span class="line"><span class="comment"># the model that will observe weight and activation tensors during calibration.</span></span><br><span class="line">model_fp32_prepared = torch.ao.quantization.prepare_qat(model_fp32_fused.train())</span><br><span class="line"></span><br><span class="line"><span class="comment"># run the training loop (not shown)</span></span><br><span class="line">training_loop(model_fp32_prepared)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert the observed model to a quantized model. This does several things:</span></span><br><span class="line"><span class="comment"># quantizes the weights, computes and stores the scale and bias value to be</span></span><br><span class="line"><span class="comment"># used with each activation tensor, fuses modules where appropriate,</span></span><br><span class="line"><span class="comment"># and replaces key operators with quantized implementations.</span></span><br><span class="line">model_fp32_prepared.<span class="built_in">eval</span>()</span><br><span class="line">model_int8 = torch.ao.quantization.convert(model_fp32_prepared)</span><br><span class="line"></span><br><span class="line"><span class="comment"># run the model, relevant calculations will happen in int8</span></span><br><span class="line">res = model_int8(input_fp32)</span><br></pre></td></tr></table></figure>

<h3 id="量化参数提取推理"><a href="#量化参数提取推理" class="headerlink" title="量化参数提取推理"></a>量化参数提取推理</h3><p>首先介绍一下torch中的qint8的tensor类型。模型使用convert()转换完成之后,模型中的权重数据类型将转为这种torch.qint8的tensor，它的整形权重，浮点权重，量化参数访问接口如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor_qint8.q_scale() <span class="comment"># S</span></span><br><span class="line">tensor_qint8.q_zero_point() <span class="comment"># Z</span></span><br><span class="line">tensor_qint8.dequantized() <span class="comment"># r</span></span><br><span class="line">tensor_qint8.int_repr() <span class="comment"># q</span></span><br></pre></td></tr></table></figure>

<p>提取得到这些量化权重和参数，便可以根据上面卷积的运算的例子进行推理部署。需要注意的是，模型中的一个层，包含层的权重$W$和层输出$Y$两组量化参数。</p>
<p>Torch底层实现这个量化卷积的过程为，输入两个qint8 tensor，通过这两个tensor的量化参数，映射到整数的量化范围（需要进行位宽钳位，部署手动实现的时候需要注意），进行卷积，得到一个32位宽的整形输出，通过输出层的量化参数，将这个32位的整形量化回到int8。</p>
<p>下面是一个py脚本，将model.state_dict()保存的量化模型权重提取保存为mat文件。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将量化训练之后的pth （model.state_dict()保存）权重，提取转换为mat</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> scipy.io <span class="keyword">as</span> sio</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">save_weights_to_mat</span>(<span class="params">net_dict: collections.OrderedDict, file_path: <span class="built_in">str</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    访问 torch模型的 net_dict 中的权重值，并将其保存为 .mat 文件。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        net_dict (collections.OrderedDict): 从 model.net_dict() 获取的权重字典。</span></span><br><span class="line"><span class="string">        file_path (str): .mat 文件的保存路径和名称 (例如 &#x27;model_weights.mat&#x27;)。</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 创建一个新的字典，用于存储 NumPy 格式的权重</span></span><br><span class="line">    mat_net_dict = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;----- 开始提取和转换权重 -----\n&quot;</span>)</span><br><span class="line">    <span class="comment"># 遍历 net_dict 中的每一层权重</span></span><br><span class="line">    <span class="keyword">for</span> key, val <span class="keyword">in</span> net_dict.items():</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;[INFO]--&gt; 处理层: <span class="subst">&#123;key&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 根据key 字符结尾分别处理. scale | zero_point | weight | bias | ._packed_params._packed_params | 其他情况跳过继续执行脚本</span></span><br><span class="line">        <span class="keyword">if</span> key.endswith(<span class="string">&quot;scale&quot;</span>):</span><br><span class="line">            key = key.replace(<span class="string">&#x27;.&#x27;</span>, <span class="string">&#x27;_&#x27;</span>)</span><br><span class="line">            mat_val = val.cpu().numpy()</span><br><span class="line">            mat_net_dict[key] = mat_val</span><br><span class="line"></span><br><span class="line">        <span class="keyword">elif</span> key.endswith(<span class="string">&quot;zero_point&quot;</span>):</span><br><span class="line">            key = key.replace(<span class="string">&#x27;.zero_point&#x27;</span>,<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">            key = key.replace(<span class="string">&#x27;.&#x27;</span>, <span class="string">&#x27;_&#x27;</span>)</span><br><span class="line">            mat_val = val.cpu().numpy()</span><br><span class="line">            mat_net_dict[<span class="string">f&quot;<span class="subst">&#123;key&#125;</span>_zp&quot;</span>] = mat_val</span><br><span class="line"></span><br><span class="line">        <span class="keyword">elif</span> key.endswith(<span class="string">&quot;weight&quot;</span>):</span><br><span class="line">            key = key.replace(<span class="string">&#x27;.weight&#x27;</span>,<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">            key = key.replace(<span class="string">&#x27;.&#x27;</span>, <span class="string">&#x27;_&#x27;</span>)</span><br><span class="line">            w_scale = np.array(val.q_scale())</span><br><span class="line">            w_zp = np.array(val.q_zero_point())</span><br><span class="line">            w_fp = val.dequantize().cpu().numpy()</span><br><span class="line">            w_int = val.int_repr().cpu().numpy()</span><br><span class="line"></span><br><span class="line">            mat_net_dict[<span class="string">f&quot;<span class="subst">&#123;key&#125;</span>_w_fp&quot;</span>] = w_fp</span><br><span class="line">            mat_net_dict[<span class="string">f&quot;<span class="subst">&#123;key&#125;</span>_w_int&quot;</span>] = w_int</span><br><span class="line">            mat_net_dict[<span class="string">f&quot;<span class="subst">&#123;key&#125;</span>_w_scale&quot;</span>] = w_scale</span><br><span class="line">            mat_net_dict[<span class="string">f&quot;<span class="subst">&#123;key&#125;</span>_w_zp&quot;</span>] = w_zp</span><br><span class="line"></span><br><span class="line">        <span class="keyword">elif</span> key.endswith(<span class="string">&quot;._packed_params._packed_params&quot;</span>):</span><br><span class="line"></span><br><span class="line">            key = key.replace(<span class="string">&#x27;._packed_params._packed_params&#x27;</span>, <span class="string">&#x27;&#x27;</span>)</span><br><span class="line">            key = key.replace(<span class="string">&#x27;.&#x27;</span>, <span class="string">&#x27;_&#x27;</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 线性层包含偏置项，考虑一个偏置问题</span></span><br><span class="line">            fc_weight = <span class="literal">None</span></span><br><span class="line">            fc_bias = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 1. 判断val的结构，安全地提取权重和偏置</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(val, <span class="built_in">tuple</span>):</span><br><span class="line">                <span class="comment"># 如果是元组，第一个元素总是权重</span></span><br><span class="line">                fc_weight = val[<span class="number">0</span>]</span><br><span class="line">                <span class="comment"># 提取偏置</span></span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">len</span>(val) &gt; <span class="number">1</span> <span class="keyword">and</span> val[<span class="number">1</span>] <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                    fc_bias = val[<span class="number">1</span>]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                fc_weight = val</span><br><span class="line"></span><br><span class="line">            w_scale = np.array(fc_weight.q_scale())</span><br><span class="line">            w_zp = np.array(fc_weight.q_zero_point())</span><br><span class="line">            w_fp = fc_weight.dequantize().cpu().numpy()</span><br><span class="line">            w_int = fc_weight.int_repr().cpu().numpy()</span><br><span class="line"></span><br><span class="line">            mat_net_dict[<span class="string">f&quot;<span class="subst">&#123;key&#125;</span>_w_fp&quot;</span>] = w_fp</span><br><span class="line">            mat_net_dict[<span class="string">f&quot;<span class="subst">&#123;key&#125;</span>_w_int&quot;</span>] = w_int</span><br><span class="line">            mat_net_dict[<span class="string">f&quot;<span class="subst">&#123;key&#125;</span>_w_scale&quot;</span>] = w_scale</span><br><span class="line">            mat_net_dict[<span class="string">f&quot;<span class="subst">&#123;key&#125;</span>_w_zp&quot;</span>] = w_zp</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> fc_bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                b_fp = fc_bias.detach().cpu().numpy()</span><br><span class="line">                mat_net_dict[<span class="string">f&quot;<span class="subst">&#123;key&#125;</span>_bias_fp&quot;</span>] = b_fp</span><br><span class="line"></span><br><span class="line">        <span class="keyword">elif</span> key.endswith(<span class="string">&quot;bias&quot;</span>):</span><br><span class="line">            <span class="comment"># 没有偏置项，不考虑提取偏置</span></span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;[INFO]--&gt; 出现没有定义的权重类型:<span class="subst">&#123;key&#125;</span>\n&quot;</span>)</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">    sio.savemat(file_path, mat_net_dict, do_compression=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;\n[INFO]--&gt; 权重已成功保存到: <span class="subst">&#123;file_path&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_model_weights</span>(<span class="params">pth_file_path</span>):</span><br><span class="line">    <span class="comment"># 从pth文件加载权重</span></span><br><span class="line">    checkpoint = torch.load(pth_file_path, map_location=<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 根据pth文件的结构选择正确的权重字典</span></span><br><span class="line">    <span class="keyword">if</span> <span class="string">&#x27;state_dict&#x27;</span> <span class="keyword">in</span> checkpoint:</span><br><span class="line">        model_weights_state_dict = checkpoint[<span class="string">&#x27;state_dict&#x27;</span>]</span><br><span class="line">    <span class="keyword">elif</span> <span class="string">&#x27;model&#x27;</span> <span class="keyword">in</span> checkpoint:</span><br><span class="line">        model_weights_state_dict = checkpoint[<span class="string">&#x27;model&#x27;</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        model_weights_state_dict = checkpoint</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model_weights_state_dict</span><br><span class="line"></span><br><span class="line"><span class="comment"># 主程序入口</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line"></span><br><span class="line">    pth_file_path = <span class="string">&quot;path/to/pth&quot;</span></span><br><span class="line"></span><br><span class="line">    model_weights_state_dict = load_model_weights(pth_file_path)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义输出文件名</span></span><br><span class="line">    output_mat_file = <span class="string">&quot;model_weights.mat&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 调用函数，传入 state_dict 和文件名</span></span><br><span class="line">    save_weights_to_mat(model_weights_state_dict, output_mat_file)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>参考<br><a href="https://zhuanlan.zhihu.com/p/505570612">模型压缩-神经网络量化基础 - 知乎</a><br><a href="https://zhuanlan.zhihu.com/p/548174416">量化感知训练（Quantization-aware-training）探索-从原理到实践 - 知乎</a><br><a href="https://leimao.github.io/article/Neural-Networks-Quantization/#Quantization">Quantization for Neural Networks - Lei Mao’s Log Book</a><br><a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Jacob_Quantization_and_Training_CVPR_2018_paper.pdf">Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference</a><br><a href="https://docs.pytorch.org/docs/stable/quantization.html">Quantization — PyTorch 2.8 documentation</a></p>
</blockquote>
]]></content>
      <tags>
        <tag>量化，QAT</tag>
      </tags>
  </entry>
</search>
